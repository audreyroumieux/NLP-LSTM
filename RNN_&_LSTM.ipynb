{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurone Normale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurone(object):\n",
    "    def __init__(self,  w=1):\n",
    "        self.w = w\n",
    "    \n",
    "    def compute(self, x):\n",
    "        return max(0, self.w * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurone Requent  (RNN)\n",
    "## $ S = relu(h_{t+1}) $\n",
    "\n",
    "## $ h_{t+1} = relu(\\theta * X_t + w * h_t) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "class RecurenceNeurone(object):\n",
    "    # constructeur\n",
    "    def __init__(self, theta=1, w=1, h=0):\n",
    "        self.theta = theta # poids\n",
    "        self.w = w\n",
    "        self.h = h # memoir interne\n",
    "        self.avtivationRelu = relu #on déclare que notre fonction d'activation est la relu\n",
    "        self.outputFunc = relu #on déclare que notre fonction de sortie est une relu\n",
    "    \n",
    "    # methode    \n",
    "    def compute(self, x):\n",
    "        z = self.theta * x + self.w * self.h\n",
    "        self.h = self.avtivationRelu(z)\n",
    "        return self.outputFunc(self.h)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurone1 = RecurenceNeurone() #creation d'un neurone qui avance de 1 en 1\n",
    "neurone1.compute(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurone1.compute(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(neurone1.compute(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "## Step by step\n",
    "### Input gate $ i_t = \\sigma(W_{i} * [h_{t-1}, x_t] + b_i)$   permet de savoir si on ecrit dans c\n",
    "\n",
    "## $\\tilde{c}_t = \\tanh(W_{ci} * [h_{t-1}, x_t] + b_c)$\n",
    "\n",
    "### Forget gate $ f_t = \\sigma (W_{f} * [h_{t-1}, x_t] + b_f)$  permet de savoir si on oublie cequ'il y avait dans c\n",
    "\n",
    "### Cell state $ c_t = f_t * c_{t-1} + i_t * \\tilde{c}_t$    \n",
    "\n",
    "### Output gate $ o_t = \\sigma (W_{ox} * [h_{t-1}, x_t] + b_o)$  \n",
    "\n",
    "###  Hidden state $ ht=o_t * tanh(c_t) $   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(np.exp(-x) + 1.0)\n",
    "    \n",
    "class LSTM_Neurone(object):\n",
    "    # Constructeur\n",
    "    def __init__(self):\n",
    "        self.h = np.array([0])# initialisation de h à 0\n",
    "        self.c = 0 # initialisation de la memoire à 0\n",
    "        \n",
    "        #initialisation des theta de i, forgate gate, c et de l'output à 1\n",
    "        self.w_i = np.array([1, 1])\n",
    "        self.w_f = np.array([1, 1])\n",
    "        self.w_c = np.array([1, 1])\n",
    "        self.w_o = np.array([1, 1])\n",
    "        \n",
    "        #on initialise les biais de i, forgate gate, c et de l'output à 0.1\n",
    "        self.b_i = 0.5\n",
    "        self.b_f = 0.5\n",
    "        self.b_c = 0.5\n",
    "        self.b_o = 0.5\n",
    "        \n",
    "        self.activation = relu\n",
    "        self.outputFunc = sigmoid\n",
    "    \n",
    "    # Methode    \n",
    "    def compute(self, x):\n",
    "        input_gate = self.activation( self.w_i.dot(np.concatenate([self.h, x]))+ self.b_i)\n",
    "        forget_gate = self.activation( self.w_f.dot(np.concatenate([self.h, x])) + self.b_f)\n",
    "        c_tilde =  np.tanh( self.w_c.dot(np.concatenate([self.h, x])) + self.b_c)\n",
    "        \n",
    "        self.o = self.outputFunc(self.w_o.dot(np.concatenate([self.h, x])) + self.b_o)\n",
    "            \n",
    "        self.c = forget_gate * self.c + input_gate * c_tilde\n",
    "        #print(\"C = \", self.c)\n",
    "        #print(\"C_tilde = \", c_tilde)\n",
    "        self.h = np.array([self.o * np.tanh(self.c)])\n",
    "        #print(\"h = \", self.h )\n",
    "        return self.o\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8175744761936437\n",
      "\n",
      "0.7713736452486172\n",
      "\n",
      "0.9058531692586362\n",
      "\n",
      "0.9172725402029648\n",
      "\n",
      "0.9181349737426983\n",
      "\n",
      "0.8050458726217169\n"
     ]
    }
   ],
   "source": [
    "nLSTM = LSTM_Neurone() #Creation d'un neurone\n",
    "print(nLSTM.compute(np.array([1])))\n",
    "print()\n",
    "print(nLSTM.compute(np.array([0])))\n",
    "print()\n",
    "print(nLSTM.compute(np.array([1])))\n",
    "print()\n",
    "print(nLSTM.compute(np.array([1])))\n",
    "print()\n",
    "print(nLSTM.compute(np.array([1])))\n",
    "print()\n",
    "print(nLSTM.compute(np.array([0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante GRU\n",
    "\n",
    "## $ c_{t+1} = f_t * c_t + (1 - f_t) * \\tilde{c}_t $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Neurone(object):\n",
    "    # Constructeur\n",
    "    def __init__(self):\n",
    "        self.h = np.array([0])# initialisation de h à 0\n",
    "        self.c = 0 # initialisation de la memoire à 0\n",
    "        \n",
    "        #initialisation des theta de i, forgate gate, c et de l'output à 1\n",
    "        self.w_f = np.array([1, 1])\n",
    "        self.w_c = np.array([1, 1])\n",
    "        self.w_o = np.array([1, 1])\n",
    "        \n",
    "        #on initialise les biais de i, forgate gate, c et de l'output à 0.5\n",
    "        self.b_f = 0.5\n",
    "        self.b_c = 0.5\n",
    "        self.b_o = 0.5\n",
    "        \n",
    "        self.activation = relu\n",
    "        self.outputFunc = sigmoid\n",
    "    \n",
    "    # Methode    \n",
    "    def compute(self, x):\n",
    "        forget_gate = self.activation( self.w_f.dot(np.concatenate([self.h, x])) + self.b_f)\n",
    "        c_tilde = np.tanh( self.w_c.dot(np.concatenate([self.h, x])) + self.b_c)\n",
    "        \n",
    "        self.o = self.outputFunc(self.w_o.dot(np.concatenate([self.h, x])) + self.b_o)\n",
    "        \n",
    "        self.c = forget_gate * self.c + (1 - forget_gate) * c_tilde\n",
    "        self.h = np.array([self.o * np.tanh(self.c)])\n",
    "        return self.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8175744761936437\n",
      "\n",
      "0.5382595861413569\n",
      "\n",
      "0.8222908789205312\n",
      "\n",
      "0.7671334523426895\n",
      "\n",
      "0.7450758037141454\n",
      "\n",
      "0.5087422726465187\n"
     ]
    }
   ],
   "source": [
    "nGRU = GRU_Neurone() #Creation d'un neurone\n",
    "print(nGRU.compute(np.array([1])))\n",
    "print()\n",
    "print(nGRU.compute(np.array([0])))\n",
    "print()\n",
    "print(nGRU.compute(np.array([1])))\n",
    "print()\n",
    "print(nGRU.compute(np.array([1])))\n",
    "print()\n",
    "print(nGRU.compute(np.array([1])))\n",
    "print()\n",
    "print(nGRU.compute(np.array([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
